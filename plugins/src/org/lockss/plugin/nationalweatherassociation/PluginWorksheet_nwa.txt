-------------------INSTRUCTIONS-------------------------------------Make a local copy of this file for each new plugin. Change its name to match the name of the plugin XML file, but with the .txt suffix. As you do your site analysis for the new plugin, follow along in this worksheet, filling in the answer to questions. Cut and paste specific URLs to support your notes and to allow subsequent readers to understand your analysis.---------------------GENERAL INFORMATION-----------------------------Name of publisher: National Weather AssociationPublishing platform: customRT: 5920RU:JIRA: PD-457Plugin name suggestion: org.lockss.plugin.nationalweatherassociation.NationalWeatherAssociationPluginClockss? [yes/no]:	noGLN? [yes/no]:			yesPLN? [and name thereof]:  TDB information:    PARAMS - global    PARAMS - journal specific    ATTRS:* params: base_url, journal_id, yearBase url:* http://www.nwas.org/Start URL(s): * http://www.nwas.org/jom/include/publications2013.phpIs the permission page in a different location? If so, where?How does the permission statement work?     (eg. text on html page, creative commons license, license that shows up     under certain conditions, single permission page)* The publisher has not put up the permission statement yet.  A single permision statement for the entire site would work. The file  lockss.txt would be fine. This is an open access site. Pick 3 AUs that you are using for initial evaluation? Write down their defined parameters. Choose AUs across a variety of journals (if available) and years to get a broad view of the publisher.(eg. Journal ID = abc; Volume = 2003; base_url = http://www.baseau-blah.org, etc)a) base_url: http://www.nwas.org/   journal_id: jom   year: 2013-----------------------URL & SITE LAYOUT SECTION-----------------------Refetch Depth (default is 1)What is the needed depth to pick up any new articles? Is this consistent for the site(s) layout? Does the publisher add items on a per-article basis or only on a per-issue basis?* Refetch Depth is 1    Crawl rules & Content LayoutIs there a predictable URL pattern specific to each type of page/content? Below is a list of possible pages and types of content. This site will probably only contain a subset. Examine the AUs you chose above and note which items are applicable and give one or more URL examples for each that exists. If you can generalize to a pattern (eg. <base_url>/<toc/<journal_id>/<volume_name>/### ) then do so.* initial crawl rules:    <list>      <string>1,"^https?://%s/.*\.(bmp|css|gif|ico|jpe?g|js|png|tif?f)$", base_url_host</string>      <string>4,"^%s", base_url</string>      <!-- http://www.nwas.org/jom/abstracts/2013/2013-JOM22/abstract.php -->      <!-- http://www.nwas.org/jom/articles/2013/2013-JOM12/2013-JOM12.pdf -->      <string>1,"^%s%s/(abstracts|articles)/%d/", base_url, journal_id, year</string>    </list>Journal Front Page (we won't collect, but need to know)* http://www.nwas.org/jom/index.phpVolume Table of Contents (May or may not be same as start url)* same as start_url  http://www.nwas.org/jom/include/publications2013.php    lists the articles for year 2013.     Issue Table of contents* this journal does not have a list of issues.Abstract:* http://www.nwas.org/jom/abstracts/2013/2013-JOM22/abstract.phpPDF* http://www.nwas.org/jom/articles/2013/2013-JOM12/2013-JOM12.pdf** pdf has links to external images - do we need link extractor?      http://www.nwas.org/jom/articles/2013/2013-JOM22-figs/Figure6.png      http://www.nwas.org/jom/articles/2013/2013-JOM22-figs/Figure7.png     (see http://www.nwas.org/jom/articles/2013/)   ==> we don't need link extractor for now (Thib).PDFPLUS* not foundPDF Landing Page(This is a page with information on it with a link to the PDF or to allow download of the PDF. This may be the same as the abstract)* not foundFull text HTML* not foundPrint friendly version option* not foundSupplementary info* not foundCitation information (also note format options - may be RIS, Bibtex, endnote, etc)* not foundReference files* not foundAudio or Video?* found links to youtube on left column, which we are not collectingImages, Figures & TablesThese items may exist in several formats, locations and with multiple access options.  First locate an article that includes images/figures/tables. If thereis a full text html option, start there. Find an embedded image (figure, table) and note the following, giving URL examples for each.* no full text html, hence no images/tables/figures for the articles.** these are images for the website from directory http://www.nwas.org/Img/:  http://www.nwas.org/Img/5u84f48n.gif	image/gif	1447  http://www.nwas.org/Img/6_new.png	image/png	11712   http://www.nwas.org/Img/LinkedIn_Logo60px.png	image/png	2544  http://www.nwas.org/Img/Logo_lg.gif	image/gif	4685  http://www.nwas.org/Img/gplus_sm.png	image/png	8775  http://www.nwas.org/Img/jrbutton.png	image/png	2748  http://www.nwas.org/Img/portal.gif	image/gif	1632  http://www.nwas.org/Img/seal.gif  http://www.nwas.org/jom/feed-icon16x16.png  http://www.nwas.org/jom/jom_banner.png	DOI patternIf this publisher uses DOIs, they may follow a pattern. It can be helpful to know what the pattern is. Give 3 examples of DOIs for this publisher including both parts (eg 101234/Blah_xxx124Is1)* not usedAddition Links to Include/ExcludeNow go back to both an issue table of contents page (or equivalent) AND an article html page and look at all the links on this page. This is easiest to do in firefox --> Page Info [Links tab]. Scan all the links and note items that might need to be explicitly included or excluded (that wouldn't be under normalpaths). If in doubt, just add an example URL in here.Links to consider excluding  underneath <base_url>, but includes something general, such as author info,   citedby, servlet, searchId, etc javascript or style sheet links that seem to   have a name that might be automatically generated (includes date or hash   string in the name, eg. <base_url>/cssJawr/N1854254985/style.css or   LEKEJEEHEJ334449595.css).* use crawl rulesLinks to consider including  not underneath <base_url>, but looks specific to article content, such as   images that live under a different base underneath <base_url> and contains   thins like showSupplements, showImage, downloadCitation)* use crawl rulesDoes this site use automatically generated content?? View the page source for an article html page and look for something in the <head> section that looks like:   __viewstate.X29DKTUELDKDHFLDKDN...   __eventstate.KDIEJTEJSDODIJGJEKE...  These types of items are generated by ASP and will change with each viewing   of the page.  Please note the URL.* not foundNotesAnything else that you think the plugin writer needs to be aware of in terms ofURL layout.--------------------------URL NORMALIZATION------------------------------------As you click around from article to article or issue TOC are you getting arguments on the end of your URLs that are likely unnecessary.  Arguments are those items that follow a ? and may be separated with &.  Arguments that are necessary to identify the page usually look like this:  ?showItem=<identifier>&format=pdfwhereas arguments that might be unnecessary might look like this  ?rss=13&prev_item=333kfkfkfjk&lang=3nThese arguments might be keeping track of browsing history or date or language. You can test whether the arguments are needed by re-entering the URL without the arguments and seeing if you get the same page. Give examples of URLs with arguments that we might need to remove. (eg. <full_url>?cookieSet=1 or <full_url>?prevSearch=3)* not needed----------------------HTML HASH FILTERING--------------------------------------Look at several types of html pages and look for types of items that are time or viewer dependent. These items will need to get hashed out. The plugin writer will need to go in to the page source to figure out how to remove the items, but you can identify which elements likely need to be removed. Here are suggestions for the type of things to look for. Make a note of the type of items you find and why you think they will need removal.  Give any URLs necessary to find the items you mention. ------Look for these sorts of things. Remove this list and replace it with  what you find---------  Name and/or logo of subscriber institution  Login identity in shopping cart area  Copyright on page showing year  Cited by section   Issue link which points to current issue  Product price listing  Footer section including adds or date  Related content or related article search  Impact factor or viewing count  Session historyAnd if you view the page source (firefox -> Page Source) look for the following:<script/> tags with subscriber information, dates, or <!-- ... --> comment pairs that includes creation or modification date sfxlink javascript commands* maximum filtering: top banner, left and right columns, and footer.-------------------HTML CRAWL FILTERING----------------------------------------If the URL pattern for articles is non-deterministic (that is, it doesn't contain volume,  year, journal id specific information) then there is no way toknow which journal/issue/volume an article is from.   deterministic URL: <base_url>/content/vol24/iss1/art1.pdf  non-deterministic URL: <base_url/content/doi/pdf/11134/myartdoi12which makes it very possible to crawl to content not from the original AU. If this is not the case, write "not applicable" or if it is, look for likely places where this could happen, such as those listed below. If you find some, please provide the URL for the page where you saw them.* not applicable. Urls are determistic:  abstract: http://www.nwas.org/jom/abstracts/2013/2013-JOM22/abstract.php  pdf: http://www.nwas.org/jom/articles/2013/2013-JOM12/2013-JOM12.pdf ------Look for these sorts of things. Remove this list and replace it with  what you find---------  Cited By Section - a link or section of links which point to articles that     reference this article  Corrigendum or Article Correction links  Related Content or Related Articles  Original Article (pointing back from a corrected article to the original)  Prev Issue/Next Issue links  Prev Article/Next Article links (sometimes these dead end at the ends of an   issue, sometimes not)-----------------------PDF FILTER--------------------------------------------Some PDF files change every time they're viewed and will require a PDF filter. Download a PDF file of an article and view it. Does it contain an obvious watermark with date downloaded or some other time/viewer specific information?In Acrobat viewer look at the properties of the file. Is the date today's date? If so it's likely to be generated on the fly. If so, note the URL of the PDF you downloaded.Now download the same PDF file a second time from a different browser (to ensure you're not getting cached version)In a terminal window go to the directory that contain these two different versions of the same PDF file and run:%diff -a pdf_version1.pdf pdf_version2.pdfIf there is a difference note that here and give the URL to the URL you tested.* not needed. I downloaded 3 pdfs and 'diff -a'.  They are the same.    2013-JOM11.pdf    2013-JOM16.pdf    2013-JOM22.pdf-----------------------METADATA EXTRACTION--------------------------------------Metadata is provided in a variety of ways depending on the publisher. For example, Atypon plugins usually provide metadata in <meta.../> tags embedded in the html of an abstract and/or full text html page.  This only concerns article pages, not TOC pages. See if this is the case. Go to an article abstract or html page and do two things:View the page info (firefox -> Page Info [General]). You should see a listing of metadata,  with names  such as dc.Title, dc. Creator, author, content_publisher, and then a value.View the page source and search for the following "<meta" there should be about as many of these as there were items listed on the General Page Info page.Other ways a publisher might provide metadata (if not as above) is as a downloadable citation file (ris, endnote, bibtex, etc). If so, please provide the format options and some examples for download.  Some publishers don't provide explicit metadata and we need to parse the source html for basic information. Is that the case?* metadata found from abstract:    title	National Weather Association    description	National Weather Association    creator	NWA IT Committee    date.created	2007-01-18    date.reviewed	2007-05-18    language	EN-US    publisher	National Weather Association    Keywords	Meteorology, Operational, Atmospheric Science,               National Weather Association, NWA    RATING	General    ROBOTS	index,follow    REVISIT-AFTER	2 weeks    ROBOTS	ALL** metadta not found: journal title, article title, author, and volume. 