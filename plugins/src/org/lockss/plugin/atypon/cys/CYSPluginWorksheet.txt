Notes: Canadian Young Scientist site uses atypon platform. Since Atypon has
been analysized, this analysis is not a full analysis. The main analysis for 
this website is crawl and hash filters.
-------------------INSTRUCTIONS-------------------------------------
Make a local copy of this file for each new plugin.
Change its name to match the name of the plugin XML file, but with the .txt suffix.
As you do your site analysis for the new plugin, follow along in this worksheet,
filling in the answer to questions. Cut and paste specific URLs to support your
notes and to allow subsequent readers to understand your analysis.
---------------------GENERAL INFORMATION-----------------------------

Name of publisher: Canadian Young Scientist
  
Publishing platform: Atypon

RT:

RU:

JIRA: PD-45

Plugin name suggestion:
* org/lockss/plugin/atypon/canadianyoungscientist/ClockssCanadianYoungScientistPlugin.xml

Clockss? [yes/no]: yes		
GLN? [yes/no]: no
PLN? [and name thereof]: no 

TDB information:
    PARAMS - global
    PARAMS - journal specific
    ATTRS:

* tdb paramsL: base_url, journal_id, volume_name

Base url: (eg. http://www.pub-name.org/)
(or does this vary by AU? If so, give multiple examples)

Start URL(s):
* http://www.cysjournal.ca/clockss/cysj/2013/index.html

Is the permission page in a different location? If so, where?
How does the permission statement work? 
    (eg. text on html page, creative commons license, license that shows up under certain conditions, single permission page)
* same as start_url http://www.cysjournal.ca/clockss/cysj/2013/index.html
  " CLOCKSS system has permission to ingest, preserve, and serve this Archival Unit. "

Pick 3 AUs that you are using for initial evaluation? Write down their defined parameters. 
Choose AUs across a variety of journals (if available) and years to get a broad view of the publisher.
(eg. Journal ID = abc; Volume = 2003; base_url = http://www.baseau-blah.org, etc)

a) base_url		http://www.cysjournal.ca/
	journal_id		cysj
	volume_name		2013

-----------------------URL & SITE LAYOUT SECTION-----------------------
Refetch Depth (default is 1)
What is the needed depth to pick up any new articles? Is this consistent for the site(s) layout? 
Does the publisher add items on a per-article basis or only on a per-issue basis?
(explanatory examples - delete this when writing up analysis) 
  If the start_url is a manifest page for an entire volume and has links for the issues in that volume, then a refetch depth of 1 would refetch 
  anything on that first level, including any new issues added to the volume since the previous crawl.  But it would not fetch any new articles
  added to an existing issue table of contents. With a refetch depth of 2 you would go on to each issue listed on the manifest page and if they
  are a table of contents with links to each article, you would see and fetch any newly added article links. 
    

Crawl rules & Content Layout
Is there a predictable URL pattern specific to each type of page/content? 
Below is a list of possible pages and types of content. This site will probably 
only contain a subset. Examine the AUs you chose above and note which items are 
applicable and give one or more URL examples for each that exists. If you can 
generalize to a pattern (eg. <base_url>/<toc/<journal_id>/<volume_name>/### ) then do so.

Journal Front Page (we won't collect, but need to know)
* http://www.cysjournal.ca/journal/cysj

Volume Table of Contents (May or may not be same as start url)
* http://www.cysjournal.ca/clockss/cysj/2013/index.html (same as start_url)

Issue Table of contents
* http://www.cysjournal.ca/toc/cysj/2013/2

Abstract
* http://www.cysjournal.ca/doi/abs/10.13034/cysj-2013-004

PDF
* http://www.cysjournal.ca/doi/pdf/10.13034/cysj-2013-004

PDFPLUS
* http://www.cysjournal.ca/doi/pdfplus/10.13034/cysj-2013-004

PDF Landing Page
* not found

Full text HTML
* http://www.cysjournal.ca/doi/full/10.13034/cysj-2013-004 

Print friendly version option

Supplementary info

Citation information (also note format options - may be RIS, Bibtex, endnote, etc)
* Ris and bib download form
  http://www.cysjournal.ca/action/showCitFormats?doi=10.13034%2Fcysj-2013-005
  

Reference files

Audio or Video?

Images, Figures & Tables
These items may exist in several formats, locations and with multiple access options.  First locate an 
article that includes images/figures/tables. If there is a full text html option, start there. Find an 
embedded image (figure, table) and note the following, giving URL examples for each.

  Are there multiple size options?
  Can it be opened in a popup?
  Does it open in its own page?
  Is it accessed via javacript (through an image viewer)?
  Does it source from a different base url?

DOI pattern
If this publisher uses DOIs, they may follow a pattern. It can be helpful to know what the 
pattern is. Give 3 examples of DOIs for this publisher including both parts (eg 101234/Blah_xxx124Is1)

a)
b)
c)

Other? Any other content specific to an article that wasn't listed above?? Give examples.

Addition Links to Include/Exclude
Now go back to both an issue table of contents page (or equivalent) AND an article html page and look at 
all the links on this page. This is easiest to do in firefox --> Page Info [Links tab].  
Scan all the links and note items that might need to be explicitly included or excluded 
(that wouldn't be under normal paths). If in doubt, just add an example URL in here.

Links to consider excluding
  underneath <base_url>, but includes something general, such as author info, citedby, servlet, searchId, etc
  javascript or style sheet links that seem to have a name that might be automatically generated (includes 
  date or hash string in the name, eg. <base_url>/cssJawr/N1854254985/style.css or LEKEJEEHEJ334449595.css).

Links to consider including
  not underneath <base_url>, but looks specific to article content, such as images that live under a different base  
  underneath <base_url> and contains thins like showSupplements, showImage, downloadCitation)

Does this site use automatically generated content?? 
View the page source for an article html page and look for something in the <head> section that looks like:
   __viewstate.X29DKTUELDKDHFLDKDN...
   __eventstate.KDIEJTEJSDODIJGJEKE...
  These types of items are generated by ASP and will change with each viewing of the page.  Please note the URL.


Notes
Anything else that you think the plugin writer needs to be aware of in terms of URL layout.

--------------------------URL NORMALIZATION---------------------------------------------
As you click around from article to article or issue TOC are you getting arguments on the end of 
your URLs that are likely unnecessary.  Arguments are those items that follow a ? and may be 
separated with &.  Arguments that are necessary to identify the page usually look like this:
  ?showItem=<identifier>&format=pdf
whereas arguments that might be unnecessary might look like this
  ?rss=13&prev_item=333kfkfkfjk&lang=3n
These arguments might be keeping track of browsing history or date or language. You can test whether the 
arguments are needed by re-entering the URL without the arguments and seeing if you get the same page.
Give examples of URLs with arguments that we might need to remove. (eg. <full_url>?cookieSet=1 or <full_url>?prevSearch=3)


----------------------HTML HASH FILTERING-----------------------------------------
Look at several types of html pages and look for types of items that are time or viewer dependent. 
These items will need to get hashed out. The plugin writer will need to go in to the page source 
to figure out how to remove the items, but you can identify which elements likely need to be removed.  
Here are suggestions for the type of things to look for. Make a note of the type of items you find 
and why you think they will need removal.  Give any URLs necessary to find the items you mention.

 ------Look for these sorts of things. Remove this list and replace it with what you find---------
  Name and/or logo of subscriber institution
  Login identity in shopping cart area
  Copyright on page showing year
  Cited by section 
  Issue link which points to current issue
  Product price listing
  Footer section including adds or date
  Related content or related article search
  Impact factor or viewing count
  Session history

And if you view the page source (firefox -> Page Source) look for the following:
<script/> tags with subscriber information, dates, or <!-- ... --> comment pairs that includes creation or modification date
sfxlink javascript commands
* use common atypon hash filter and nearly identical to NRCResearchPress.
  - header, footer, scripts, comments, banner, nav-wrapper (Home, About, etc.),
  breadcrumbs, 'Also Read', spider link, whole left sidebar, whole right
  sidebar

Anything else you think might need to be removed from hashing??


-------------------HTML CRAWL FILTERING----------------------------------------
If the URL pattern for articles is non-deterministic (that is, it doesn't contain volume,  year, journal 
id specific information) then there is no way to know which journal/issue/volume an article is from. 
 deterministic URL: <base_url>/content/vol24/iss1/art1.pdf
 non-deterministic URL: <base_url/content/doi/pdf/11134/myartdoi12
which makes it very possible to crawl to content not from the original AU. If this is not the case, 
write "not applicable" or if it is, look for likely places where this could happen, such as those 
listed below. If you find some, please provide the URL for the page where you saw them.

 ------Look for these sorts of things. Remove this list and replace it with what you find---------
  Cited By Section - a link or section of links which point to articles that reference this article
  Corrigendum or Article Correction links
  Related Content or Related Articles
  Original Article (pointing back from a corrected article to the original)
  Prev Issue/Next Issue links
  Prev Article/Next Article links (sometimes these dead end at the ends of an issue, sometimes not)
* use common atypon crawl filter and nearly identical to NRCResearchPress.
  - nav-wrapper (Home, About, etc.), 'Also Read', spider link, 
    whole left sidebar, whole right, lisf of issues link.
  - can not filter the whole rihgt sidebar since it has Citation Downloads
    which we want to collect

-----------------------PDF FILTER--------------------------------------------
Some PDF files change every time they're viewed and will require a PDF filter. 
Download a PDF file of an article and view it. Does it contain an obvious watermark with date 
downloaded or some other time/viewer specific information?
In Acrobat viewer look at the properties of the file. Is the date today's date? 
If so it's likely to be generated on the fly. 
If so, note the URL of the PDF you downloaded.

Now download the same PDF file a second time from a different browser (to ensure you're not getting cached version)

In a terminal window go to the directory that contain these two different versions of the same PDF file and run:
%diff -a pdf_version1.pdf pdf_version2.pdf
If there is a difference note that here and give the URL to the URL you tested.

** pdfplus has growing list of references (note pull-down blue triangle symbol)
http://www.cysjournal.ca/doi/pdfplus/10.13034/cysj-2013-001
http://www.cysjournal.ca/doi/pdfplus/10.13034/cysj-2013-008

-----------------------METADATA EXTRACTION--------------------------------------
Metadata is provided in a variety of ways depending on the publisher. 
For example, Atypon plugins usually provide metadata in <meta.../> tags embedded in the html of 
an abstract and/or full text html page.  This only concerns article pages, not TOC pages.  
See if this is the case. Go to an article abstract or html page and do two things:

View the page info (firefox -> Page Info [General]). 
You should see a listing of metadata,  with names  such as dc.Title, dc. Creator, author, content_publisher, and then a value.

View the page source and search for the following "<meta" 
there should be about as many of these as there were items listed on the General Page Info page.

Other ways a publisher might provide metadata (if not as above) is as a downloadable citation file 
(ris, endnote, bibtex, etc). If so, please provide the format options and some examples for download.  

Some publishers don't provide explicit metadata and we need to parse the source html for basic information. Is that the case?

