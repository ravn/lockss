Name of publisher: ElsevierPublishing platform?: File transfer - XML schema is Elsevier specificRT: 6626 or 4922JIRA:PD-1214This is a new delivery format form Elsevier. It turns out that it's not JATS as we had thought, but just a later version of a proprietary Elsevier DTD, so we are naming the pluginElsevierDTD5.see this link for more information: http://www.elsevier.com/author-schemas/elsevier-xml-dtds-and-transport-schemasHow is this content delivered to us?FTP - need to get more information here from Clay. He is doing some stuff to make sure we de-dup between bucket years.Plugin name: ClockssElsevierDTD5SourcePluginThis content representsJournals - currently Books - coming? this is a little tricky because the schema is significantly different.  See additional notes below.Books-series - would be handled like journalsWill be using the basic ClockssSourcePlugin framework with XML parsing for the metadata information.CONTENT LAYOUT:Each delivery includes a set of tar files (all associated with one file number), and a <fileno>.ready.xml file identifying the tar files in the delivery. Each tar set, when unpacked, would yield a directory <fileno>/ under which all the contents would live. At the top of the directory structure is a file "dataset.xml" which identifies all the included files, using the Elsevier DTD5 Schema.note:The plugin currently supports Journals and Book-Series because the top level "dataset.xml" is the same for these. If we also get delivery of books, it would be simplest to handle it through a different delivery location so we could use a slightly different plugin since the top level "dataset.xml" for books is different. It might be possible to modify the existing plugin to handle both, but there is no way of knowing which schema to use without opening the dataset.xml file.For example:    CLKS0000000000001.ready.xml    CLKS0000000000001A.tar    CLKS0000000000001B.tar    CLKS0000000000001C.tar Would unpack in to a directory structure    CLKS0000000000001/        dataset.xml        01420615/v64sC/S0142061514004608/main.pdf, main.xml, ....        01694332/v317sC/S0169433214020108/main.pdf, main.xml, ...        <issn>/<vol-issue>/<articleid>/article_specific_contents where the lowest level "main.xml" and "main.pdf" represent specific articles within a given issue/volume/publication The top level dataset.xml includes general metadata for each article along with relative paths to the article files, but only the "main.xml" article-level XML file contains the article title and author information.The new plugin is a child of the ClockssSourcePlugin, with its own schema and some additional support to handle pulling metadata from two locations.The article iterator finds all the CLKS<fileno>A.tar/<fileno>/dataset.xml files. The schema looks for journal_items within the dataset.xml file and extracts raw article metadata, as well as the specific dtd for the underlying "article" (currently options are ARTICLE, SIMPLE-ARTICLE, or BOOK-REVIEW), and relative links to the article's main.pdf and main.xml. The MetadataExtractor then goes over the entire list of raw ArticleMetadata objects and creates a map from relative article paths to specific paths within the CU (including which of the tar balls in the tar set the file lives in). Then, before emitting, the extractor verifies the existence of the identified PDF file, and opens the associated "main.xml" file to extract the final pieces of necessary metadata - author information and article title - before cooking and emitting. SCHEMA INFORMATION:dataset.xml contains<dataset>  <dataset_content>     <journal_issue>       <journal_item.../>       <journal_item.../>       ...     </journal_issue>     <journal_issue>          etc...  </dataset_content></dataset>for every single article in the delivery.Each journal_item represents an "article". We pull the item date, relative pathname (to main.xml and main.pdf), type of pdf, journal issue, volume and publication title from this one file and store as raw-data.We also pull each articles "dtd-version" from this file and store it as raw-data.The starting node for the contents of the underlying main.xml depend on the dtd stored in raw-metadata, but the underlying items are "common elements" among all dtds and can share a schema definition.we pick up "ce:author-group" and "ce:title"The top node is one of:"/article/head"; "/simple-article/simple-head"; or "/book-review/book-review-head";    note: if this plugin has to handle "book" as well, we'll need to add support.